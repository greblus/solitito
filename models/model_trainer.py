# -*- coding: utf-8 -*-
"""Soltito_AI_trainer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1forK75nPVog_vKSYxh_7u0hb42KLKWti
"""

# --- 0. INSTALACJA ---
!pip install onnx onnxscript jams librosa tqdm

import os
import zipfile
import requests
import jams
import librosa
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import torch.onnx
from tqdm import tqdm
import glob
from scipy.signal import sawtooth

# --- 1. KONFIGURACJA ---
SAMPLE_RATE = 44100
FFT_SIZE_PYTHON = 16384
START_MIDI = 40
NUM_BINS = 48

ROOT_NAMES = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]
CHORD_TYPES = {
    "": [0, 4, 7], "m": [0, 3, 7],
    "Maj7": [0, 4, 7, 11], "m7": [0, 3, 7, 10], "7": [0, 4, 7, 10],
    "m7b5": [0, 3, 6, 10], "dim7": [0, 3, 6, 9]
}

LABELS = []
for r in ROOT_NAMES:
    for t in CHORD_TYPES.keys():
        if t == "": LABELS.append(r)
        else: LABELS.append(f"{r} {t}")
for r in ROOT_NAMES:
    LABELS.append(f"Note {r}")
LABELS.append("Noise")
LABELS.sort()

NUM_CLASSES = len(LABELS)
LABEL_TO_IDX = {label: i for i, label in enumerate(LABELS)}
print(f"Klasy: {NUM_CLASSES}")

# --- 2. POBIERANIE ---
def download_file(url, filename):
    if os.path.exists(filename) and os.path.getsize(filename) > 1024*1024:
        print(f"Plik {filename} już istnieje.")
        return True
    if os.path.exists(filename): os.remove(filename)
    print(f"Pobieranie {filename}...")
    try:
        response = requests.get(url, stream=True)
        if response.status_code != 200: return False
        with open(filename, 'wb') as file, tqdm(total=int(response.headers.get('content-length', 0)), unit='iB', unit_scale=True) as bar:
            for data in response.iter_content(1024*1024):
                file.write(data)
                bar.update(len(data))
        return True
    except: return False

def prepare_dataset():
    if not os.path.exists("guitarset_audio"): os.makedirs("guitarset_audio")
    if not os.path.exists("guitarset_annotations"): os.makedirs("guitarset_annotations")

    # Audio
    if not glob.glob("guitarset_audio/**/*.wav", recursive=True):
        if download_file("https://zenodo.org/records/3371780/files/audio_mono-pickup_mix.zip?download=1", "audio.zip"):
            print("Rozpakowywanie Audio...")
            with zipfile.ZipFile("audio.zip", 'r') as zip_ref: zip_ref.extractall("guitarset_audio")

    # Adnotacje
    if not glob.glob("guitarset_annotations/**/*.jams", recursive=True):
        if download_file("https://zenodo.org/records/3371780/files/annotation.zip?download=1", "annotation.zip"):
            print("Rozpakowywanie Adnotacji...")
            with zipfile.ZipFile("annotation.zip", 'r') as zip_ref: zip_ref.extractall("guitarset_annotations")

prepare_dataset()

# --- 3. DSP (48 bins) ---
def compute_spectrogram_48(audio_segment):
    n_fft = FFT_SIZE_PYTHON
    if len(audio_segment) < n_fft:
        audio_segment = np.pad(audio_segment, (0, n_fft - len(audio_segment)))

    spectrum = np.abs(np.fft.rfft(audio_segment, n=n_fft))
    freqs = np.fft.rfftfreq(n_fft, d=1/SAMPLE_RATE)
    bins = np.zeros(NUM_BINS, dtype=np.float32)

    for i, mag in enumerate(spectrum):
        f = freqs[i]
        if f > 50 and f < 1400:
            midi = 12 * np.log2(f / 440.0) + 69
            idx = int(round(midi)) - START_MIDI
            if 0 <= idx < NUM_BINS: bins[idx] += mag

    # Log + Norm (Zgodne z Rust)
    if bins.max() > 0: bins /= bins.max()
    bins = np.log1p(bins * 10.0)
    if bins.max() > 0: bins /= bins.max()
    return bins

def generate_waveform(midi_notes, duration=0.15):
    t = np.linspace(0, duration, int(SAMPLE_RATE * duration), endpoint=False)
    wave = np.zeros_like(t)
    for midi in midi_notes:
        freq = 440.0 * (2.0 ** ((midi - 69) / 12.0))
        freq *= (2.0 ** (np.random.normal(0, 0.04) / 12.0))
        wave += sawtooth(2 * np.pi * freq * t) * np.random.uniform(0.4, 1.0)
    wave += np.random.normal(0, 0.005, len(t))
    return wave

def map_jams_chord_to_label(chord_str):
    if chord_str == "N": return None
    try: root, qualities = chord_str.split(':')
    except: return None
    root = root.replace("b", "#")
    if root == "Eb": root = "D#"
    if root == "Bb": root = "A#"
    if root == "Ab": root = "G#"
    if root == "Gb": root = "F#"
    if root == "Db": root = "C#"

    target_qual = None
    if "maj7" in qualities: target_qual = "Maj7"
    elif "min7" in qualities or "m7" in qualities: target_qual = "m7"
    elif "7" in qualities and "maj" not in qualities and "min" not in qualities: target_qual = "7"
    elif "hdim" in qualities: target_qual = "m7b5"
    elif "dim7" in qualities: target_qual = "dim7"
    elif "maj" in qualities: target_qual = ""
    elif "min" in qualities: target_qual = "m"

    if target_qual is not None and root in ROOT_NAMES:
        if target_qual == "": return root
        return f"{root} {target_qual}"
    return None

# --- 4. DATASET ---
def build_dataset(max_real_samples=80000, synth_samples=20000):
    X = []
    y = []
    print("Przetwarzanie GuitarSet...")
    audio_files = sorted(glob.glob("guitarset_audio/**/*.wav", recursive=True))
    jams_root = "guitarset_annotations/annotation"
    if not os.path.exists(jams_root): jams_root = "guitarset_annotations"

    real_count = 0
    for audio_path in tqdm(audio_files):
        if real_count >= max_real_samples: break
        basename = os.path.basename(audio_path)
        jams_name = basename.replace("_mix.wav", ".jams").replace(".wav", ".jams")
        found_jams = glob.glob(f"{jams_root}/**/{jams_name}", recursive=True)
        if not found_jams: continue
        jams_path = found_jams[0]
        try:
            y_audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE, mono=True)
            jam = jams.load(jams_path)
            ann = jam.annotations.search(namespace='chord')[0]
            for obs in ann.data:
                label = map_jams_chord_to_label(obs.value)
                if label and label in LABEL_TO_IDX:
                    start = int(obs.time * sr)
                    end = int((obs.time + obs.duration) * sr)
                    mid = (start + end) // 2
                    half_window = FFT_SIZE_PYTHON // 2
                    if end - start > FFT_SIZE_PYTHON:
                        for offset in [-half_window, half_window]:
                            if mid + offset + FFT_SIZE_PYTHON < len(y_audio):
                                seg = y_audio[mid+offset : mid+offset+FFT_SIZE_PYTHON]
                                X.append(compute_spectrogram_48(seg))
                                y.append(LABEL_TO_IDX[label])
                                real_count += 1
        except: continue
    print(f"Zebrano {real_count} real.")

    print("Generowanie synth...")
    for _ in range(synth_samples):
        label_idx = np.random.randint(0, NUM_CLASSES)
        label_name = LABELS[label_idx]
        midi_notes = []
        if label_name == "Noise": pass
        elif "Note" in label_name:
            root_str = label_name.split(' ')[1]
            root_val = ROOT_NAMES.index(root_str)
            octave = np.random.choice([3, 4, 5])
            note = root_val + 12 * octave
            if note < 40: note += 12
            midi_notes.append(note)
            if np.random.rand() > 0.7: midi_notes.append(note + 7)
        else:
            parts = label_name.split(' ')
            root_str = parts[0]
            qual = parts[1] if len(parts) > 1 else ""
            root_val = ROOT_NAMES.index(root_str)
            intervals = CHORD_TYPES[qual]
            base_midi = root_val + 12 * np.random.choice([3, 4])
            if base_midi < 40: base_midi += 12
            for interval in intervals:
                note = base_midi + interval
                if interval > 0 and np.random.rand() > 0.5: note += 12
                midi_notes.append(note)

        spec = compute_spectrogram_48(generate_waveform(midi_notes))
        spec += np.random.normal(0, 0.02, NUM_BINS)
        X.append(np.clip(spec, 0.0, 1.0))
        y.append(label_idx)

    return np.array(X, dtype=np.float32), np.array(y, dtype=np.longlong)

X_all, y_all = build_dataset()
if len(X_all) > 0:
    idx = np.arange(len(X_all))
    np.random.shuffle(idx)
    X_all = X_all[idx]
    y_all = y_all[idx]
    split = int(0.8 * len(X_all))
    X_train, y_train = torch.from_numpy(X_all[:split]), torch.from_numpy(y_all[:split])
    X_test, y_test = torch.from_numpy(X_all[split:]), torch.from_numpy(y_all[split:])

    # --- 5. MODEL (POPRAWIONY: Rozpłaszczony) ---
    class ChordNet48(nn.Module):
        def __init__(self):
            super(ChordNet48, self).__init__()
            # Definiujemy warstwy osobno
            self.fc1 = nn.Linear(48, 1024)
            self.bn1 = nn.BatchNorm1d(1024)
            self.relu1 = nn.ReLU()
            self.drop1 = nn.Dropout(0.4)

            self.fc2 = nn.Linear(1024, 512)
            self.bn2 = nn.BatchNorm1d(512)
            self.relu2 = nn.ReLU()
            self.drop2 = nn.Dropout(0.4)

            self.fc3 = nn.Linear(512, 256)
            self.bn3 = nn.BatchNorm1d(256)
            self.relu3 = nn.ReLU()

            self.fc4 = nn.Linear(256, NUM_CLASSES)

        def forward(self, x):
            x = self.fc1(x)
            x = self.bn1(x)
            x = self.relu1(x)
            x = self.drop1(x)

            x = self.fc2(x)
            x = self.bn2(x)
            x = self.relu2(x)
            x = self.drop2(x)

            x = self.fc3(x)
            x = self.bn3(x)
            x = self.relu3(x)

            x = self.fc4(x)
            return x

    model = ChordNet48()
    if torch.cuda.is_available():
        model = model.cuda()
        X_train, y_train = X_train.cuda(), y_train.cuda()
        X_test, y_test = X_test.cuda(), y_test.cuda()

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0002)

    print("Trening...")
    for epoch in range(100):
        model.train()
        optimizer.zero_grad()
        out = model(X_train)
        loss = criterion(out, y_train)
        loss.backward()
        optimizer.step()
        if (epoch+1)%10==0:
            model.eval()
            with torch.no_grad():
                test_out = model(X_test)
                _, pred = torch.max(test_out, 1)
                acc = (pred == y_test).sum().item() / len(y_test)
            print(f'Ep {epoch+1}, Acc: {acc*100:.2f}%')

    model.eval()
    model.to('cpu')
    dummy = torch.randn(10, 48)
    torch.onnx.export(model, dummy, "chord_model.onnx", export_params=True, opset_version=17,
                      input_names=['input'], output_names=['output'],
                      dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})
    print("Gotowe.")