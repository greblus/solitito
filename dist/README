How the model was trained?

dataset_dual_generator.py generates dataset_dual.gp5 - dual because it has 
two tracks: one with triads, jazz shapes and single notes, the other one with 
binary representation of chord index in dataset_reference beeped with a synth saw sound.
These beeps are then detected to create perfect annotations.

Sounds crazy? It is - for some strange reason I had troubles creating correctly syncd 
dataset_annotations.csv with some more mathematic methods. Maybe it's W11 ;) maybe my 
DAW of choice, no es importa :)

decode_annotations.py takes beeps_render.wav file, exported from 2nd track of dataset_dual.gp5 
and dataset_reference.csv with proper names names of sound samples and creates 
dataset_annotations.csv.

Now the fun part no 2: the 1st track from gp5 file is exported as "DI" guitar signal,
raw_render.wav which is then rendered in DAW through NAM plugin into dataset_clean.wav 
(Fender Deluxe Reverb clean sound) and dataset_eob.wav (Fender Deluxe Reverb edge of breakup 
tone). Doesn't matter how stupid it seems - I could create lots of nice training data 
almost automatically ;)

And the last part: model_trainer.py takes dataset_clean.wav, dataset_eob.wav (and also 
another dataset called GuitarSet, which is currently disabled to reduce training time)
and dataset_annotations.csv and splits the datasets accordingly for training following 
the naming and timing from this csv file.



