# -*- coding: utf-8 -*-
"""Solitito_9.3_KaggleInput_CRNN.ipynb"""

# 1. INSTALACJA
!pip install -q onnx onnxscript jams librosa torch tqdm

import os
import glob
import requests
import zipfile
import jams
import librosa
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import torch.onnx
import gc
from tqdm import tqdm
from torch.utils.data import DataLoader, TensorDataset

# --- KONFIGURACJA ---
SAMPLE_RATE = 22050
FFT_SIZE = 2048
HOP_LENGTH = 512

# CQT Params
F_MIN = 65.4        
BINS_PER_OCTAVE = 36
N_OCTAVES = 6
LOG_BINS = BINS_PER_OCTAVE * N_OCTAVES # 216

# Czas
CTX_FRAMES = 16
BATCH_SIZE = 256

# Klasy
ROOTS = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]
QUALS = ["", "m", "Maj7", "m7", "7", "dim7", "m7b5"] 
LABELS = [f"{r} {q}".strip() for r in ROOTS for q in QUALS] + ["Noise"]
for r in ROOTS: LABELS.append(f"Note {r}")

LABEL_TO_IDX = {l: i for i, l in enumerate(LABELS)}
NUM_CLASSES = len(LABELS)
OUTPUT_DIR = "/kaggle/working/"

print(f"Model Config | Input: {LOG_BINS}x{CTX_FRAMES} | Classes: {NUM_CLASSES}")

# --- 2. DSP (Pseudo-CQT) ---
def compute_log_spectrogram(audio):
    if len(audio) < FFT_SIZE: audio = np.pad(audio, (0, FFT_SIZE - len(audio)))
    spec = np.abs(np.fft.rfft(audio, n=FFT_SIZE))
    
    log_spec = np.zeros(LOG_BINS, dtype=np.float32)
    hz_per_bin = SAMPLE_RATE / FFT_SIZE
    
    for i in range(LOG_BINS):
        center_freq = F_MIN * (2.0 ** (i / BINS_PER_OCTAVE))
        fft_idx = int(round(center_freq / hz_per_bin))
        if 0 <= fft_idx < len(spec):
            val = spec[fft_idx]
            # Smoothing
            if fft_idx > 0: val += spec[fft_idx-1] * 0.5
            if fft_idx < len(spec)-1: val += spec[fft_idx+1] * 0.5
            log_spec[i] = val

    log_spec = np.log1p(log_spec)
    if log_spec.max() > 0: log_spec /= log_spec.max()
    return log_spec

# --- 3. PARSOWANIE NAZW (Twoje pliki) ---
def parse_custom_filename(path):
    # Przykady: 
    # "Am_clean.wav" -> "A m"
    # "Asharpm_blues.wav" -> "A# m"
    # "single_notes_Asharp_blues.wav" -> "Note A#"
    
    filename = os.path.basename(path)
    name = filename.replace(".wav", "")
    
    # A. POJEDYNCZE NUTY
    if name.startswith("single_notes_"):
        # Usuwamy prefix i suffixy
        clean = name.replace("single_notes_", "").split('_')[0] # np. "Asharp"
        
        # Zamiana "sharp" na "#"
        root = clean.replace("sharp", "#")
        
        label = f"Note {root}"
        if label in LABEL_TO_IDX: return LABEL_TO_IDX[label]
        # print(f"Ignored Note: {label}")
        return None

    # B. AKORDY
    else:
        clean = name.split('_')[0] # np. "Asharpm"
        
        # Najpierw zamiemy "sharp" na "#", 偶eby atwiej parsowa
        clean = clean.replace("sharp", "#") # "A#m"
        
        # Detekcja Root i Quality
        root = ""
        qual = ""
        
        # Jeli drugi znak to # (np. A#m)
        if len(clean) > 1 and clean[1] == '#':
            root = clean[:2]
            rest = clean[2:]
        else:
            root = clean[:1]
            rest = clean[1:]
            
        if rest == "m": qual = "m"
        elif rest == "": qual = ""
        elif rest == "dim": qual = "dim7" # Opcjonalnie
        else:
            return None # Nieznany typ

        label = f"{root} {qual}".strip()
        if label in LABEL_TO_IDX: return LABEL_TO_IDX[label]
        return None

# --- 4. MAPOWANIE JAMS (GuitarSet) ---
def map_jams(chord_str):
    if chord_str == "N": return "Noise"
    try:
        r, q = chord_str.split(':')
        r = r.replace('b','#').replace('Eb','D#').replace('Bb','A#').replace('Ab','G#').replace('Db','C#').replace('Gb','F#')
        target = None
        if "dim" in q: target="dim7"
        elif "hdim" in q: target="m7b5"
        elif "maj7" in q: target="Maj7"
        elif "min7" in q or "m7" in q: target="m7"
        elif "7" in q: target="7"
        elif "maj" in q: target=""
        elif "min" in q or "m" in q: target="m"
        if target is not None:
            l = f"{r} {target}".strip()
            if l in LABEL_TO_IDX: return l
    except: pass
    return None

# --- 5. BUDOWANIE DATASETU ---
def build_dataset():
    print("\n=== BUDOWANIE DATASETU ===")
    X_seq = []
    y_seq = []
    
    # --- A. TWJ DATASET Z KAGGLE INPUT ---
    # Szukamy rekurencyjnie w /kaggle/input/
    print("Szukanie Twoich plik贸w w /kaggle/input/...")
    custom_files = glob.glob("/kaggle/input/**/*.wav", recursive=True)
    
    # Filtrujemy, 偶eby nie wzio GuitarSet (jeli by tam by)
    custom_files = [f for f in custom_files if "guitarset" not in f.lower()]
    
    print(f"Znaleziono {len(custom_files)} Twoich plik贸w.")
    
    # Augmentacja dla Twoich plik贸w (偶eby zwikszy ich ilo i odporno)
    # Pitch shift: 0, -0.3, +0.3 p贸tonu
    CUSTOM_SHIFTS = [0, -0.3, 0.3]

    for af in tqdm(custom_files, desc="Custom Data"):
        label_idx = parse_custom_filename(af)
        if label_idx is None: continue
        
        try:
            y_orig, sr = librosa.load(af, sr=SAMPLE_RATE, mono=True)
            y_trimmed, _ = librosa.effects.trim(y_orig, top_db=30)
            
            for shift in CUSTOM_SHIFTS:
                if shift == 0: y = y_trimmed
                else: y = librosa.effects.pitch_shift(y_trimmed, sr=sr, n_steps=shift)
                
                # Spectrogram
                S = []
                for i in range(0, len(y)-FFT_SIZE, HOP_LENGTH // 2): # Gsty overlap dla Twoich danych
                    if i+FFT_SIZE > len(y): break
                    chunk = y[i:i+FFT_SIZE]
                    S.append(compute_log_spectrogram(chunk))
                
                if len(S) < CTX_FRAMES: continue
                S = np.array(S)
                
                # Tniemy gsto
                for t in range(0, len(S) - CTX_FRAMES, 4):
                    window = S[t : t+CTX_FRAMES]
                    X_seq.append(window)
                    y_seq.append(label_idx)
                    
        except Exception as e: print(f"Err {af}: {e}")

    # --- B. GUITARSET (To) ---
    # Pobieramy GuitarSet do /kaggle/working/, 偶eby uzupeni braki (np. akordy 7, maj7, kt贸rych nie nagrae)
    if not os.path.exists("guitarset_audio"):
        print("Pobieranie GuitarSet...")
        try:
            r = requests.get("https://zenodo.org/records/3371780/files/audio_mono-pickup_mix.zip?download=1", stream=True)
            with open("audio.zip", "wb") as f:
                for chunk in r.iter_content(chunk_size=1024*1024): f.write(chunk)
            with zipfile.ZipFile("audio.zip", "r") as z: z.extractall("guitarset_audio")
            
            r = requests.get("https://zenodo.org/records/3371780/files/annotation.zip?download=1", stream=True)
            with open("annot.zip", "wb") as f:
                for chunk in r.iter_content(chunk_size=1024*1024): f.write(chunk)
            with zipfile.ZipFile("annot.zip", "r") as z: z.extractall("guitarset_annotations")
        except: pass

    gs_files = sorted(glob.glob("guitarset_audio/**/*.wav", recursive=True))
    jams_root = "guitarset_annotations/annotation" if os.path.exists("guitarset_annotations/annotation") else "guitarset_annotations"
    
    print(f"Przetwarzanie {len(gs_files)} plik贸w GuitarSet...")
    for af in tqdm(gs_files, desc="GuitarSet"):
        try:
            y_orig, sr = librosa.load(af, sr=SAMPLE_RATE, mono=True)
            fname = os.path.basename(af).replace("_mix.wav", ".jams")
            jp = glob.glob(f"{jams_root}/**/{fname}", recursive=True)
            if not jp: continue
            ann = jams.load(jp[0]).annotations.search(namespace='chord')[0]
            
            # Bez augmentacji pitch dla GuitarSet (oszczdno czasu), chyba 偶e masz zapas czasu
            S = []
            for i in range(0, len(y_orig)-FFT_SIZE, HOP_LENGTH):
                S.append(compute_log_spectrogram(y_orig[i:i+FFT_SIZE]))
            S = np.array(S)
            
            # Rzadsze cicie (co 16 klatek)
            for t in range(0, len(S) - CTX_FRAMES, 16): 
                window = S[t : t+CTX_FRAMES]
                mid_time = (t + CTX_FRAMES//2) * HOP_LENGTH / SAMPLE_RATE
                lbl_idx = LABEL_TO_IDX["Noise"]
                for obs in ann.data:
                    if obs.time <= mid_time <= obs.time + obs.duration:
                        l = map_jams(obs.value)
                        if l: lbl_idx = LABEL_TO_IDX[l]
                        break
                X_seq.append(window)
                y_seq.append(lbl_idx)
        except: pass
        gc.collect()

    # --- C. NOISE ---
    print("Generowanie szumu...")
    target_noise = int(len(X_seq) * 0.1)
    for _ in range(target_noise):
        window = np.random.normal(0, 0.02, (CTX_FRAMES, LOG_BINS)).astype(np.float32)
        window = np.log1p(np.abs(window))
        X_seq.append(window)
        y_seq.append(LABEL_TO_IDX["Noise"])

    return np.array(X_seq, dtype=np.float32), np.array(y_seq, dtype=np.longlong)

X_all, y_all = build_dataset()

if len(X_all) == 0: raise RuntimeError("Brak danych! Sprawd藕 cie偶ki input.")

# Dataset & Loader
ds = TensorDataset(torch.from_numpy(X_all), torch.from_numpy(y_all))
train_size = int(0.85 * len(ds))
test_size = len(ds) - train_size
tr_ds, te_ds = torch.utils.data.random_split(ds, [train_size, test_size])

train_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
test_loader = DataLoader(te_ds, batch_size=BATCH_SIZE, num_workers=2)

print(f"Gotowe. Trening na {len(tr_ds)} pr贸bkach.")

# --- 6. MODEL CRNN ---
class CRNN(nn.Module):
    def __init__(self):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32), nn.ReLU(),
            nn.MaxPool2d((1, 2)), 
            
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64), nn.ReLU(),
            nn.MaxPool2d((1, 2)),
            
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128), nn.ReLU(),
            nn.MaxPool2d((1, 2)), 
        )
        self.gru = nn.GRU(input_size=128*27, hidden_size=128, num_layers=2, batch_first=True, dropout=0.3)
        self.fc = nn.Linear(128, NUM_CLASSES)

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.conv(x) 
        b, c, t, f = x.size()
        x = x.permute(0, 2, 1, 3).contiguous().view(b, t, c*f)
        _, hn = self.gru(x)
        x = hn[-1]
        x = self.fc(x)
        return x

# --- 7. TRENING ---
model = CRNN()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.device_count() > 1:
    print(f" Multi-GPU: {torch.cuda.device_count()}")
    model = nn.DataParallel(model)
model.to(device)

crit = nn.CrossEntropyLoss()
opt = optim.Adam(model.parameters(), lr=0.001)
sched = optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.5)

print("Start Treningu...")
for ep in range(30): # 30 epok
    model.train()
    tl = 0
    for X, y in train_loader:
        X, y = X.to(device), y.to(device)
        opt.zero_grad()
        out = model(X)
        loss = crit(out, y)
        loss.backward()
        opt.step()
        tl += loss.item()
    
    sched.step()
    avg = tl/len(train_loader)
    print(f"Ep {ep+1}: Loss {avg:.4f}")
    
    # Zapis
    if isinstance(model, nn.DataParallel): m_save = model.module
    else: m_save = model
    
    path = os.path.join(OUTPUT_DIR, f"chord_model_ep{ep+1}.onnx")
    m_save.eval().cpu()
    torch.onnx.export(m_save, torch.randn(1, CTX_FRAMES, LOG_BINS), path, 
                      input_names=['input'], output_names=['output'],
                      dynamic_axes={'input':{0:'b'}, 'output':{0:'b'}},
                      optimize=False)
    m_save.to(device)
    print(f"-> Saved: {path}")

print("KONIEC.")
