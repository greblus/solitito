# -*- coding: utf-8 -*-
"""Solitito_Trainer_v9.0.ipynb"""

# 1. INSTALACJA
!pip install -q onnx onnxscript jams librosa torch tqdm

import os
import glob
import requests
import zipfile
import jams
import librosa
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import torch.onnx
import gc
import shutil
from tqdm import tqdm
from torch.utils.data import DataLoader, TensorDataset
from scipy.signal import sawtooth

# --- KONFIGURACJA SOTA ---
SAMPLE_RATE = 22050
FFT_SIZE = 2048
HOP_LENGTH = 512

# CQT Params (Zgodne z Rustem)
F_MIN = 65.4
BINS_PER_OCTAVE = 36
N_OCTAVES = 6
LOG_BINS = BINS_PER_OCTAVE * N_OCTAVES # 216

# Czas
CTX_FRAMES = 16
BATCH_SIZE = 256

# Klasy (Zgodne z Rust Brain)
ROOTS = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]
QUALS = ["", "m", "Maj7", "m7", "7", "dim7", "m7b5"] 
LABELS = [f"{r} {q}".strip() for r in ROOTS for q in QUALS] + ["Noise"]
for r in ROOTS: LABELS.append(f"Note {r}")

LABEL_TO_IDX = {l: i for i, l in enumerate(LABELS)}
NUM_CLASSES = len(LABELS)
OUTPUT_DIR = "/kaggle/working/"

print(f"Model Config | Input: {LOG_BINS}x{CTX_FRAMES} | Classes: {NUM_CLASSES}")

# --- 2. POBIERANIE GUITARSET ---
def prepare_guitarset():
    if not os.path.exists("guitarset_audio"):
        print("Pobieranie GuitarSet Audio...")
        try:
            r = requests.get("https://zenodo.org/records/3371780/files/audio_mono-pickup_mix.zip?download=1", stream=True)
            with open("audio.zip", "wb") as f:
                for chunk in r.iter_content(chunk_size=1024*1024):
                    if chunk: f.write(chunk)
            with zipfile.ZipFile("audio.zip", "r") as z: z.extractall("guitarset_audio")
        except Exception as e: print(f"GuitarSet DL Error: {e}")

    if not os.path.exists("guitarset_annotations"):
        print("Pobieranie GuitarSet Annotations...")
        try:
            r = requests.get("https://zenodo.org/records/3371780/files/annotation.zip?download=1", stream=True)
            with open("annot.zip", "wb") as f:
                for chunk in r.iter_content(chunk_size=1024*1024):
                    if chunk: f.write(chunk)
            with zipfile.ZipFile("annot.zip", "r") as z: z.extractall("guitarset_annotations")
        except Exception as e: print(f"GuitarSet Annot Error: {e}")

prepare_guitarset()

# --- 3. DSP (ZGODNE Z RUST) ---
def compute_log_spectrogram(audio):
    if len(audio) < FFT_SIZE: audio = np.pad(audio, (0, FFT_SIZE - len(audio)))
    spec = np.abs(np.fft.rfft(audio, n=FFT_SIZE))
    
    log_spec = np.zeros(LOG_BINS, dtype=np.float32)
    hz_per_bin = SAMPLE_RATE / FFT_SIZE
    
    for i in range(LOG_BINS):
        center_freq = F_MIN * (2.0 ** (i / BINS_PER_OCTAVE))
        fft_idx = int(round(center_freq / hz_per_bin))
        if 0 <= fft_idx < len(spec):
            val = spec[fft_idx]
            # Smoothing
            if fft_idx > 0: val += spec[fft_idx-1] * 0.5
            if fft_idx < len(spec)-1: val += spec[fft_idx+1] * 0.5
            log_spec[i] = val

    log_spec = np.log1p(log_spec)
    if log_spec.max() > 0: log_spec /= log_spec.max()
    return log_spec

# --- 4. PARSOWANIE NAZW PLIK√ìW I JAMS ---
def map_jams(chord_str):
    if chord_str == "N": return "Noise"
    try:
        r, q = chord_str.split(':')
        r = r.replace('b','#').replace('Eb','D#').replace('Bb','A#').replace('Ab','G#').replace('Db','C#').replace('Gb','F#')
        target = None
        if "dim" in q: target="dim7"
        elif "hdim" in q: target="m7b5"
        elif "maj7" in q: target="Maj7"
        elif "min7" in q or "m7" in q: target="m7"
        elif "7" in q: target="7"
        elif "maj" in q: target=""
        elif "min" in q or "m" in q: target="m"
        
        if target is not None:
            l = f"{r} {target}".strip()
            if l in LABEL_TO_IDX: return l
    except: pass
    return None

def parse_idmt_filename(path):
    # Format IDMT: "Gmaj_1.wav"
    name = os.path.basename(path).split('.')[0]
    parts = name.split('_')
    chord_code = parts[0]
    
    # Rozdziel Root i Quality
    if len(chord_code) > 1 and chord_code[1] == '#':
        root = chord_code[:2]
        qual_code = chord_code[2:]
    elif len(chord_code) > 1 and chord_code[1] == 'b':
        # Bemole na krzy≈ºyki
        root_raw = chord_code[:2]
        root = root_raw.replace('Eb','D#').replace('Bb','A#').replace('Ab','G#').replace('Db','C#').replace('Gb','F#')
        qual_code = chord_code[2:]
    else:
        root = chord_code[:1]
        qual_code = chord_code[1:]

    qual = None
    if qual_code == "maj": qual = ""
    elif qual_code == "min": qual = "m"
    elif qual_code == "maj7": qual = "Maj7"
    elif qual_code == "min7": qual = "m7"
    elif qual_code == "7": qual = "7"
    
    if qual is None: return None
    
    label = f"{root} {qual}".strip()
    return LABEL_TO_IDX.get(label)

def gen_note_wave(root):
    try: idx = ROOTS.index(root)
    except: return np.zeros(FFT_SIZE)
    t = np.linspace(0, FFT_SIZE/SAMPLE_RATE, FFT_SIZE, endpoint=False)
    midi = 12 * np.random.choice([2,3,4]) + idx + 12
    if midi < 40: midi += 12
    f = 440.0 * (2**((midi-69)/12))
    w = sawtooth(2*np.pi*f*t) * 0.5 + np.sin(2*np.pi*f*t) * 0.5
    return w.astype(np.float32)

# --- 5. BUDOWANIE DATASETU ---
def build_dataset():
    print("\n=== BUDOWANIE MEGADATASETU ===")
    X_seq = []
    y_seq = []
    
    # Augmentacja Pitch (0, -0.35, +0.35 p√≥≈Çtonu)
    SHIFT_STEPS = [0, -0.35, 0.35]

    # --- ≈πR√ìD≈ÅO 1: GuitarSet ---
    gs_files = sorted(glob.glob("guitarset_audio/**/*.wav", recursive=True))
    jams_root = "guitarset_annotations/annotation" if os.path.exists("guitarset_annotations/annotation") else "guitarset_annotations"
    
    print(f"GuitarSet: Znaleziono {len(gs_files)} plik√≥w.")
    
    for af in tqdm(gs_files, desc="GuitarSet"):
        try:
            y_orig, sr = librosa.load(af, sr=SAMPLE_RATE, mono=True)
            fname = os.path.basename(af).replace("_mix.wav", ".jams")
            jp = glob.glob(f"{jams_root}/**/{fname}", recursive=True)
            if not jp: continue
            ann = jams.load(jp[0]).annotations.search(namespace='chord')[0]
            
            for shift in SHIFT_STEPS:
                if shift == 0: y = y_orig
                else: y = librosa.effects.pitch_shift(y_orig, sr=sr, n_steps=shift)
                
                # Spectrogram
                S = []
                for i in range(0, len(y)-FFT_SIZE, HOP_LENGTH):
                    S.append(compute_log_spectrogram(y[i:i+FFT_SIZE]))
                S = np.array(S)
                
                # Slicing
                for t in range(0, len(S) - CTX_FRAMES, 8):
                    mid_time = (t + CTX_FRAMES//2) * HOP_LENGTH / SAMPLE_RATE
                    lbl_idx = LABEL_TO_IDX["Noise"]
                    for obs in ann.data:
                        if obs.time <= mid_time <= obs.time + obs.duration:
                            l = map_jams(obs.value)
                            if l: lbl_idx = LABEL_TO_IDX[l]
                            break
                    
                    X_seq.append(S[t : t+CTX_FRAMES])
                    y_seq.append(lbl_idx)
        except Exception as e: pass
        gc.collect()

    # --- ≈πR√ìD≈ÅO 2: IDMT-SMT-GUITAR (Je≈õli dostƒôpne) ---
    # Szukamy w typowych ≈õcie≈ºkach Kaggle/Upload
    idmt_paths = [
        "/kaggle/input/idmt-smt-guitar-v2/IDMT-SMT-GUITAR_V2/dataset2/audio",
        "idmt_audio/dataset2/audio", # Lokalnie/Colab
        "IDMT-SMT-GUITAR_V2/dataset2/audio"
    ]
    idmt_files = []
    for p in idmt_paths:
        found = glob.glob(f"{p}/**/*.wav", recursive=True)
        if found:
            idmt_files = found
            print(f"IDMT: Znaleziono {len(found)} plik√≥w w {p}")
            break
            
    if idmt_files:
        for af in tqdm(idmt_files, desc="IDMT Processing"):
            try:
                lbl_idx = parse_idmt_filename(af)
                if lbl_idx is None: continue # Pomijamy nieznane typy
                
                y_orig, sr = librosa.load(af, sr=SAMPLE_RATE, mono=True)
                
                # Augmentacja IDMT (te≈º warto!)
                for shift in SHIFT_STEPS:
                    if shift == 0: y = y_orig
                    else: y = librosa.effects.pitch_shift(y_orig, sr=sr, n_steps=shift)

                    S = []
                    for i in range(0, len(y)-FFT_SIZE, HOP_LENGTH):
                        S.append(compute_log_spectrogram(y[i:i+FFT_SIZE]))
                    S = np.array(S)
                    
                    # IDMT to pojedyncze uderzenia. Bierzemy kilka okien ze ≈õrodka.
                    if len(S) > CTX_FRAMES:
                        # Bierzemy max 3 okna z nagrania (poczƒÖtek, ≈õrodek, zanikanie)
                        step = max(1, (len(S)-CTX_FRAMES)//3)
                        for t in range(0, len(S)-CTX_FRAMES, step):
                             X_seq.append(S[t : t+CTX_FRAMES])
                             y_seq.append(lbl_idx)
            except: pass
            gc.collect()
    else:
        print("IDMT: Nie znaleziono datasetu. Trenujƒô bez niego.")

    # --- ≈πR√ìD≈ÅO 3: SYNTH (Balans) ---
    # Celujemy w 30% udzia≈Çu nut
    total_real = len(X_seq)
    target_synth = max(2000, int(total_real * 0.3))
    print(f"Generowanie {target_synth} nut syntetycznych...")
    
    for _ in range(target_synth):
        root = np.random.choice(ROOTS)
        lbl = LABEL_TO_IDX[f"Note {root}"]
        wav = gen_note_wave(root)
        spec = compute_log_spectrogram(wav)
        window = np.tile(spec, (CTX_FRAMES, 1))
        X_seq.append(window)
        y_seq.append(lbl)
        
    # Noise
    print("Generowanie szumu...")
    for _ in range(target_synth // 2):
        window = np.random.normal(0, 0.05, (CTX_FRAMES, LOG_BINS)).astype(np.float32)
        window = np.log1p(np.abs(window))
        X_seq.append(window)
        y_seq.append(LABEL_TO_IDX["Noise"])

    print(f"RAZEM: {len(X_seq)} sekwencji treningowych.")
    return np.array(X_seq, dtype=np.float32), np.array(y_seq, dtype=np.longlong)

X_all, y_all = build_dataset()

if len(X_all) == 0: raise RuntimeError("Pusty dataset!")

# Tensor Dataset
ds = TensorDataset(torch.from_numpy(X_all), torch.from_numpy(y_all))
train_size = int(0.85 * len(ds))
test_size = len(ds) - train_size
tr_ds, te_ds = torch.utils.data.random_split(ds, [train_size, test_size])

# DataLoaders
train_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
test_loader = DataLoader(te_ds, batch_size=BATCH_SIZE, num_workers=2)

# --- 6. MODEL CRNN ---
class CRNN(nn.Module):
    def __init__(self):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32), nn.ReLU(),
            nn.MaxPool2d((1, 2)), 
            
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64), nn.ReLU(),
            nn.MaxPool2d((1, 2)),
            
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128), nn.ReLU(),
            nn.MaxPool2d((1, 2)), # Freq 27
        )
        self.gru = nn.GRU(input_size=128*27, hidden_size=128, num_layers=2, batch_first=True, dropout=0.3)
        self.fc = nn.Linear(128, NUM_CLASSES)

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.conv(x) 
        b, c, t, f = x.size()
        x = x.permute(0, 2, 1, 3).contiguous().view(b, t, c*f)
        _, hn = self.gru(x)
        x = hn[-1]
        x = self.fc(x)
        return x

# --- 7. TRENING ---
model = CRNN()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.device_count() > 1:
    print(f"üöÄ Multi-GPU: {torch.cuda.device_count()}")
    model = nn.DataParallel(model)
model.to(device)

crit = nn.CrossEntropyLoss()
opt = optim.Adam(model.parameters(), lr=0.001)
sched = optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.5)

print("Start Treningu...")
for ep in range(20):
    model.train()
    tl = 0
    for X, y in train_loader:
        X, y = X.to(device), y.to(device)
        opt.zero_grad()
        out = model(X)
        loss = crit(out, y)
        loss.backward()
        opt.step()
        tl += loss.item()
    
    sched.step()
    avg = tl/len(train_loader)
    print(f"Ep {ep+1}: Loss {avg:.4f}")
    
    # Zapis Checkpointu
    if isinstance(model, nn.DataParallel): m_save = model.module
    else: m_save = model
    
    path = os.path.join(OUTPUT_DIR, f"chord_model_ep{ep+1}.onnx")
    m_save.eval().cpu()
    torch.onnx.export(m_save, torch.randn(1, CTX_FRAMES, LOG_BINS), path, 
                      input_names=['input'], output_names=['output'],
                      dynamic_axes={'input':{0:'b'}, 'output':{0:'b'}},
                      optimize=False)
    m_save.to(device)
    print(f"-> Saved: {path}")

print("KONIEC.")
